# -*- coding: utf-8 -*-
# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
#          Manuel Moussallam <manuel.moussallam@gmail.com>
#
# Algorithm presented here are described in:
# Blind Denoising with Random Greedy Pursuits.
# Moussallam, M., Gramfort, A., Daudet, L., & Richard, G. (2014).
# IEEE Signal Processing Letters, 21(11), 1341ï¿½1345
#
# License: BSD (3-clause)

from math import sqrt
import multiprocessing
from functools import partial

import numpy as np
from scipy.special import erfinv
from scipy import linalg

from joblib import Parallel, delayed, Memory

from .mdct_tools import mdct_waveform, mdct, MDCT


def check_random_state(seed):
    """Turn seed into a np.random.RandomState instance

    If seed is None, return the RandomState singleton used by np.random.
    If seed is an int, return a new RandomState instance seeded with seed.
    If seed is already a RandomState instance, return it.
    Otherwise raise ValueError.
    """
    if seed is None or seed is np.random:
        return np.random.mtrand._rand
    if isinstance(seed, (int, np.integer)):
        return np.random.RandomState(seed)
    if isinstance(seed, np.random.RandomState):
        return seed
    raise ValueError(
        "%r cannot be used to seed a numpy.random.RandomState"
        " instance" % seed
    )


def _single_mp_run(
    x,
    Phi,
    bound,
    max_iter,
    verbose=False,
    pad=0,
    random_state=None,
    memory=Memory(None),
):
    """ run of the RSSMP algorithm """

    rng = check_random_state(random_state)
    pad = int(pad)
    x = np.concatenate((np.zeros(pad), x, np.zeros(pad)))

    n = x.size
    m = Phi.doth(x).size
    err_mse = []

    # Initialisation
    residual = np.concatenate((x.copy(), np.zeros(max(Phi.sizes) // 2)))

    s = np.zeros(m)
    x_est = np.zeros(n)
    # Main algorithm
    coeffs = np.zeros(m)
    it_number = 0
    current_lambda = 1
    err_mse.append(linalg.norm(residual))

    # Decomposition loop: stopping criteria is either SNR or iteration number
    while (current_lambda > bound) & (it_number < max_iter):

        # pick a shift at random : in each size
        rndshifts = []
        for scale_idx, size in enumerate(Phi.sizes):
            shift = rng.randint(low=0, high=size // 4)
            coeffs[scale_idx * n: (scale_idx + 1) * n] = mdct(
                residual[shift: shift + n], size
            ).ravel()
            rndshifts.append(shift)

        # Select a new element
        idx = np.argmax(np.abs(coeffs))

        # Update coefficients
        s[idx] += coeffs[idx]

        # Only one method now : local update via a cached waveform
        # find scale and frequency bin of selected atom
        mdct_wf = memory.cache(mdct_waveform)

        scale_idx = idx // n
        size = Phi.sizes[scale_idx]
        F = n // (size // 2)
        frame = (idx - (scale_idx * n)) % F
        freq_bin = ((idx - (scale_idx * n))) // F
        pos = (frame * size // 2) - size // 4 + rndshifts[scale_idx]
        residual[pos: pos + size] -= coeffs[idx] * mdct_wf(size, freq_bin)

        # also add it to the reconstruction
        x_est[pos: pos + size] += coeffs[idx] * mdct_wf(size, freq_bin)

        # error computation (err_mse)
        err_mse.append(linalg.norm(residual))

        current_lambda = np.sqrt(1 - err_mse[-1] / err_mse[-2])
        if current_lambda <= bound:
            x_est[pos: pos + size] -= coeffs[idx] * mdct_wf(size, freq_bin)
        if verbose:
            print(
                "Iteration %d : Current lambda of %1.4f"
                % (it_number, current_lambda)
            )
        it_number += 1

    return x_est, err_mse


def _single_multichannel_mp_run(
    X,
    Phi,
    bound,
    selection_rule,
    stop_crit,
    max_iter,
    verbose=False,
    pad=0,
    random_state=None,
    memory=Memory(None),
):
    """ run of the structured variant of the RSSMP algorithm """
    rng = check_random_state(random_state)

    # padding as v stak
    pad = int(pad)
    n_channels = X.shape[0]
    X = np.hstack(
        (np.zeros((n_channels, pad)), X, np.zeros((n_channels, pad)))
    )
    n_samples = X.shape[1]
    n_projs = Phi.doth(X).shape[1]
    err_mse = {}

    # Initialisation
    residual = np.hstack((X.copy(),
                          np.zeros((n_channels, max(Phi.sizes) // 2))))

    s_rep = np.zeros((n_channels, n_projs))
    X_est = np.zeros((n_channels, n_samples))
    # Main algorithm
    coeffs = np.zeros((n_channels, n_projs))

    it_number = 0
    current_lambda = 1
    for c_idx in range(n_channels):
        err_mse[c_idx] = []
        err_mse[c_idx].append(linalg.norm(residual[c_idx, :]))

    # Decomposition loop: stopping criteria is either SNR or iteration number
    while (current_lambda > bound) & (it_number < max_iter):

        # pick a shift at random : in each size
        rndshifts = {}
        for c_idx in range(n_channels):
            rndshifts[c_idx] = []
        for s_idx, L in enumerate(Phi.sizes):
            shift = rng.randint(low=0, high=L // 4)
            for c_idx in range(n_channels):
                coeffs[
                    c_idx, s_idx * n_samples: (s_idx + 1) * n_samples
                ] = mdct(residual[c_idx, shift: shift + n_samples], L).ravel()
                rndshifts[c_idx].append(shift)

        # Multichannel mode : we combine projections
        combined = selection_rule(coeffs ** 2)

        # Select a new element
        idx = np.argmax(np.abs(combined))
        # find scale and frequency bin of selected atom
        s_idx = idx // n_samples
        L = Phi.sizes[s_idx]
        F = n_samples // (L // 2)
        frame = (idx - (s_idx * n_samples)) % F
        freq_bin = ((idx - (s_idx * n_samples))) // F

        mdct_wf = memory.cache(mdct_waveform)

        # Update coefficients and residual
        current_lambda_array = np.zeros(n_channels)
        for c_idx in range(n_channels):
            s_rep[c_idx, idx] += coeffs[c_idx, idx]

            # Only one method now : local update via a cached waveform
            pos = (frame * L // 2) - L // 4 + rndshifts[c_idx][s_idx]
            residual[c_idx, pos: pos + L] -= coeffs[c_idx, idx] * mdct_wf(
                L, freq_bin
            )

            # also add it to the reconstruction
            X_est[c_idx, pos: pos + L] += coeffs[c_idx, idx] * mdct_wf(
                L, freq_bin
            )

            # error computation (err_mse)
            err_mse[c_idx].append(linalg.norm(residual[c_idx, :]))

            current_lambda_array[c_idx] = np.sqrt(
                1.0 - err_mse[c_idx][-1] / err_mse[c_idx][-2]
            )

        current_lambda = stop_crit(current_lambda_array)

        if verbose:
            print(
                "Iteration %d : Current lambda of %1.4f"
                % (it_number, current_lambda)
            )
        it_number += 1

    return X_est[:, pad:-pad], err_mse


def _pad(X):
    """add zeroes on the border to make sure the signal length is a
    power of two"""
    p_above = int(np.floor(np.log2(X.shape[1])))
    M = 2 ** (p_above + 1) - X.shape[1]
    X = np.hstack((np.zeros((X.shape[0], M)), X))

    return X, M


def _denoise(
    seeds,
    x,
    dico,
    sup_bound,
    n_atoms,
    verbose=False,
    indep=True,
    stop_crit=None,
    selection_rule=None,
    pad=0,
    memory=Memory(None),
):
    """multiple rssmp runs with a smart stopping criterion using
    the convergence decay monitoring
    """
    approx = []
    for seed in seeds:
        if verbose > 0:
            print("Run seed %d" % seed)
        if indep:
            approx.append(
                _single_mp_run(
                    x,
                    dico,
                    sup_bound,
                    n_atoms,
                    verbose=verbose,
                    pad=pad,
                    random_state=seed,
                    memory=memory,
                )[0]
            )
        else:
            approx.append(
                _single_multichannel_mp_run(
                    x,
                    dico,
                    sup_bound,
                    selection_rule,
                    stop_crit,
                    n_atoms,
                    verbose=verbose,
                    pad=pad,
                    random_state=seed,
                    memory=memory,
                )[0]
            )
    return approx


def _bird_core(
    X,
    scales,
    n_runs,
    Lambda_W,
    max_iter=100,
    stop_crit=np.mean,
    selection_rule=np.sum,
    n_jobs=1,
    indep=True,
    random_state=None,
    memory=Memory(None),
    verbose=False,
):
    """Automatically detect when noise zone has been reached and stop
    MP at this point

    Parameters
    ----------
    X : array, shape (n_channels, n_times)
        The data to be denoised where n_channels is number of sensors
        and n_times the dimension
    scales : list
        The list of MDCT scales that will be used to built the
        dictionary Phi
    n_runs : int
        the number of runs (n_runs in the paper)
    Lambda_W : float
        bound for lambda under which a run will be stopped
    max_iter : int
        Maximum number of iterations (serves as alternate stopping criterion)
    stop_crit : function
        controls the calculation of Lambda
    selection_rule : callable
        controls the way multiple channel projections are combined for atom
        selection only used if indep=False
    n_jobs : int
        number of jobs to run in parallel
    indep : bool
        True for BIRD (independent processing of each channel,
        False for S-BIRD (structured sparsity seeked)
    random_state : None | int | np.random.RandomState
        To specify the random generator state (seed).
    memory : instance of Memory
        The object to use to cache some computations. If cachedir is None, no
        caching is performed.
    verbose : bool
        verbose mode

    Returns
    -------
    X_denoise : array, shape (n_channels, n_times)
        denoised array of same shape as X
    """
    Phi = MDCT(scales)
    pad = int(1.5 * max(scales))
    X_denoise = np.zeros_like(X)
    approx = []
    rng = check_random_state(random_state)
    seeds = rng.randint(4294967295, size=n_runs)  # < max seed value

    if n_jobs <= 0:
        n_cores = multiprocessing.cpu_count()
        n_jobs = min(n_cores + n_jobs + 1, n_cores)

    if indep:
        # Independent treat of each channel (plain BIRD)
        for r, x in zip(X_denoise, X):
            this_approx = Parallel(n_jobs=n_jobs)(
                delayed(_denoise)(
                    this_seeds,
                    x,
                    Phi,
                    Lambda_W,
                    max_iter,
                    pad=pad,
                    verbose=verbose,
                    indep=True,
                    memory=memory,
                )
                for this_seeds in np.array_split(seeds, n_jobs)
            )
            this_approx = sum(this_approx[1:], this_approx[0])
            r[:] = sum([a[pad:-pad] for a in this_approx])
            approx.append(this_approx)
    else:
        # data need to be processed jointly
        this_approx = Parallel(n_jobs=n_jobs)(
            delayed(_denoise)(
                this_seeds,
                X,
                Phi,
                Lambda_W,
                max_iter,
                pad=pad,
                verbose=verbose,
                selection_rule=selection_rule,
                indep=False,
                memory=memory,
                stop_crit=stop_crit,
            )
            for this_seeds in np.array_split(seeds, n_jobs)
        )

        # reconstruction by averaging
        for jidx in range(len(this_approx)):
            for ridx in range(len(this_approx[jidx])):
                X_denoise += this_approx[jidx][ridx]

    X_denoise /= float(n_runs)
    return X_denoise


def bird(
    X,
    scales,
    n_runs,
    p_above,
    max_iter=100,
    random_state=None,
    n_jobs=1,
    memory=Memory(None),
    verbose=False,
):
    """The BIRD algorithm as described in the paper

    Parameters
    ----------
    X : array, shape (n_channels, n_times)
        The numpy n_channels-vy-N array to be X_denoised where n_channels
        is number of sensors and n_times the dimension
    scales : list
        The list of MDCT scales that will be used to built the
        dictionary Phi
    n_runs : int
        the number of runs (n_runs in the paper)
    p_above : float
        probability of appearance of the max above which the noise hypothesis
        is considered false
    max_iter : int
        The maximum number of iterations in one pursuit.
    random_state : None | int | np.random.RandomState
        To specify the random generator state (seed).
    max_iter : int
        The maximum number of iterations in one pursuit.
    n_jobs : int
        The number of jobs to run in parallel.
    memory : instance of Memory
        The object to use to cache some computations. If cachedir is None, no
        caching is performed.
    verbose : bool
        verbose mode

    Returns
    -------
    X_denoise : array, shape (n_channels, n_times)
        The X_denoised data.
    """
    X, prepad = _pad(X)

    # Computing Lambda_W(Phi, p_above)
    N = float(X.shape[1])
    # size of the full shift-invariant dictionary
    M = np.sum(np.array(scales) / 2) * N
    sigma = sqrt((1.0 - (2.0 / np.pi)) / float(N))
    Lambda_W = sigma * sqrt(2.0) * erfinv((1.0 - p_above) ** (1.0 / float(M)))
    print(
        "Starting BIRD with MDCT dictionary of %d Atoms. "
        "Lambda_W=%1.3f, n_runs=%d" % (M, Lambda_W, n_runs)
    )
    X_denoised = _bird_core(
        X,
        scales,
        n_runs,
        Lambda_W,
        verbose=verbose,
        max_iter=max_iter,
        indep=True,
        n_jobs=n_jobs,
        random_state=random_state,
        memory=memory,
    )
    return X_denoised[:, prepad:]


# the stopping criterion is determined by the p_active parameter
def stop_crit(lambda_array, lint):
    lambda_array.sort()
    return np.mean(lambda_array[-int(lint):])


def selection_rule(projections_matrix, lint):
    sorted_projs = np.sort(projections_matrix ** 2, axis=0)
    return np.mean(sorted_projs[-lint:, :], axis=0)


def s_bird(
    X,
    scales,
    n_runs,
    p_above,
    p_active=1,
    max_iter=100,
    random_state=None,
    n_jobs=1,
    memory=Memory(None),
    verbose=False,
):
    """Multichannel version of BIRD (S-BIRD) seeking Structured Sparsity

    Parameters
    ----------
     X : array, shape (n_channels, n_times)
        The numpy n_channels-vy-n_samples array to be denoised where n_channels
         is the number of sensors and n_samples the dimension
    scales : list of int
        The list of MDCT scales that will be used to built the
        dictionary Phi
    n_runs : int
        the number of runs (n_runs in the paper)
    p_above : float
        probability of appearance of the max above which the noise hypothesis
        is considered false
    p_active : float
        proportion of active channels (l in the paper)
    max_iter : int
        The maximum number of iterations in one pursuit.
    random_state : None | int | np.random.RandomState
        To specify the random generator state (seed).
    n_jobs : int
        The number of jobs to run in parallel.
    memory : instance of Memory
        The object to use to cache some computations. If cachedir is None, no
        caching is performed.
    verbose : bool
        verbose mode

    Returns
    -------
    X_denoise : array, shape (n_channels, n_times)
        The denoised data.
    """
    X, prepad = _pad(X)
    # Computing Lambda_W(Phi, p_above)
    n_channels = X.shape[0]
    n_samples = float(X.shape[1])
    # size of the full shift-invariant dictionary
    M = np.sum(np.array(scales) // 2) * n_samples
    sigma = sqrt((1.0 - (2.0 / np.pi)) / float(n_samples))
    Lambda_W = sigma * sqrt(2.0) * erfinv((1.0 - p_above) ** (1.0 / float(M)))

    lint = int(n_channels * p_active)

    this_stop_crit = partial(stop_crit, lint=lint)  # XXX : check lint here
    this_selection_rule = partial(selection_rule, lint=lint)

    print(
        "Starting S-BIRD with MDCT dictionary of %d Atoms."
        " Lambda_W=%1.3f, n_runs=%d, p_active=%1.1f"
        % (M, Lambda_W, n_runs, p_active)
    )
    denoised = _bird_core(
        X,
        scales,
        n_runs,
        Lambda_W,
        verbose=verbose,
        stop_crit=this_stop_crit,
        n_jobs=n_jobs,
        selection_rule=this_selection_rule,
        max_iter=max_iter,
        indep=False,
        memory=memory,
    )

    return denoised[:, prepad:]
